{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI ChatGPT & Prompt Engineering for Generative AI Prose\n",
    "- For this, you'll need to store your *Open AI Key* in your local **.env** file\n",
    "- **OPENAI_API_KEY**=\\<your key\\>\n",
    "- https://platform.openai.com/account/api-keys\n",
    "- *IPython.display.Markdown* is used\n",
    "\n",
    "\n",
    "## openai.Completion.create ()\n",
    "\n",
    "**model** - Specifies the model to use. Different models have different capabilities and costs.\n",
    "- *Valid Range*: Model IDs provided by OpenAI (e.g., text-davinci-003, text-curie-001).\n",
    "- *Example*: model=\"text-davinci-003\"\n",
    "\n",
    "**prompt** - The input text to which the model will respond. It can be a question, statement, or any text snippet. The model generates its completion based on this prompt.\n",
    "- *Valid Range*: Any string.\n",
    "- *Example*: prompt=\"What is the capital of France?\"\n",
    "\n",
    "**max_tokens** - The maximum number of tokens (words or parts of words) to generate in the completion. The larger the number, the longer the response.\n",
    "- *Valid Range*: An integer, typically up to 4096 for most models.\n",
    "- *Example*: max_tokens=150\n",
    "\n",
    "**temperature** - Controls randomness in the generation. A low temperature (close to 0) makes the model more deterministic and repetitive, while a high temperature (closer to 1) makes it more creative and random.\n",
    "- *Valid Range*: A float between 0 and 1.\n",
    "- *Example*: temperature=0.7\n",
    "\n",
    "**top_p** - This parameter, also known as ***nucleus sampling***, controls the diversity of the generated text. It takes a value between 0 and 1, with higher values leading to more diversity.\n",
    "- *Valid Range*: A float between 0 and 1.\n",
    "- *Example*: top_p=0.9\n",
    "\n",
    "**n** - The number of completions to generate for the given prompt. Useful for generating multiple different responses to the same prompt.\n",
    "- *Valid Range*: An integer, usually small (e.g., 1-10).\n",
    "- *Example*: n=3\n",
    "\n",
    "**stream** - If set to ***true***, the API sends results as a stream, meaning you'll get partial results before the full completion is generated.\n",
    "- *Valid Range*: Boolean (true or false).\n",
    "Example: stream=true\n",
    "\n",
    "**logprobs** - Specifies the number of log probabilities to return. Useful for understanding the model's decision-making process.\n",
    "- *Valid Range*: An integer, typically between 0 and 100.\n",
    "- *Example*: logprobs=10\n",
    "\n",
    "**echo** - If set to ***true***, the response includes the original prompt as well as the completion.\n",
    "- *Valid Range*: Boolean (true or false).\n",
    "- *Example*: echo=true\n",
    "\n",
    "**stop** - A sequence of tokens that, when encountered, tells the model to stop generating further tokens. Useful for defining a clear endpoint for the generated text.\n",
    "- *Valid Range*: Any string or an array of strings.\n",
    "- *Example*: stop=[\"\\n\", \"END\"]\n",
    "\n",
    "**presence_penalty** and **frequency_penalty** - These parameters discourage the model from repeating the same information (presence penalty) or from using overly frequent words and phrases (frequency penalty).\n",
    "- *Valid Range*: A float typically between 0 and 1.\n",
    "- *Example*: presence_penalty=0.5, frequency_penalty=0.5\n",
    "\n",
    "**best_of** - Asks the model to generate multiple completions internally and return the best one. Can be used to improve the quality of the output at the cost of increased token usage.\n",
    "- *Valid Range*: An integer, usually greater than 1.\n",
    "- *Example*: best_of=5\n",
    "\n",
    "**user** - An identifier for the user making the request. Useful for filtering logs and understanding usage patterns.\n",
    "- *Valid Range*: Any string that uniquely identifies the user.\n",
    "- *Example*: user=\"user_12345\"\n",
    "\n",
    "---\n",
    "\n",
    "**Prompt**: Explain quantum entanglement \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from IPython.display import Markdown, display\n",
    "from langchain.llms import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "# Pull out environment variables for API config & set LangChain env\n",
    "load_dotenv()\n",
    "OAIkey    = os.getenv(\"OPENAI_API_KEY\")\n",
    "# print (f\"OPENAI_API_KEY='{OAIkey}'\\n\")\n",
    "\n",
    "try:\n",
    "    get_ipython ()\n",
    "    inJupyter = True\n",
    "     \n",
    "except:\n",
    "    inJupyter = False\n",
    "    \n",
    "print (f\" inJupyter = {inJupyter}\\n\")\n",
    "# os.environ[\"OPENAI_API_KEY\"] = OAIkey\n",
    "# Set Open AI parameters...\n",
    "def setOpenAIparams (\n",
    "    model             = \"text-davinci-003\",\n",
    "    temperature       = 0.7,\n",
    "    max_tokens        = 256,\n",
    "    top_p             = 1,\n",
    "    n                 = 3,\n",
    "    frequency_penalty = 0,\n",
    "    presence_penalty  = 0 ):\n",
    " \n",
    "    openai_params = {}    \n",
    "    openai_params['model']             = model\n",
    "    openai_params['temperature']       = temperature\n",
    "    openai_params['max_tokens']        = max_tokens\n",
    "    openai_params['top_p']             = top_p\n",
    "    openai_params['n']                 = n\n",
    "    openai_params['frequency_penalty'] = frequency_penalty\n",
    "    openai_params['presence_penalty']  = presence_penalty\n",
    "    return openai_params\n",
    "\n",
    "# Get Open AI completion... \n",
    "def getOpenAIcompletion(params, prompt):\n",
    "    \n",
    "    openai.api_key = OAIkey\n",
    "    response       = openai.Completion.create (\n",
    "        engine            = params['model'],\n",
    "        prompt            = prompt,\n",
    "        temperature       = params['temperature'],\n",
    "        max_tokens        = params['max_tokens'],\n",
    "        top_p             = params['top_p'],\n",
    "        n                 = params['n'],\n",
    "        frequency_penalty = params['frequency_penalty'],\n",
    "        presence_penalty  = params['presence_penalty'] )\n",
    "    return response\n",
    "    \n",
    "# Test Open AI\n",
    "def testOpenAI ():\n",
    "    \n",
    "    yourInput  = ''\n",
    "    iCount     = 0\n",
    "    params     = setOpenAIparams ()\n",
    "    \n",
    "    # Solicit input until it is correct.\n",
    "    while(yourInput == ''):\n",
    "    \tyourInput = input(\"\\Open AI Prompt ---> \") \n",
    "    \n",
    "    #  Get a response from Open AI\n",
    "    try:\n",
    "        response = getOpenAIcompletion(params, yourInput)\n",
    "        print (response.choices)\n",
    "        \n",
    "    except Exception as err:\n",
    "        print(f\"\\n\\n*** Unexpected Error ***\\n\\n{err=}\\n\\n{type(err)=}\")\n",
    "        return\n",
    "    \n",
    "    # Display response(s)\n",
    "    if inJupyter:\n",
    "        display (Markdown (\"# Top Response(s)  \\n\"))\n",
    "        \n",
    "    else:\n",
    "        print (\"\\nTop Response(s)\\n\")\n",
    "       \n",
    "    for OAIresponseChoice in response.choices: \n",
    "        responseText = OAIresponseChoice.text\n",
    "        \n",
    "        if inJupyter:\n",
    "            display (Markdown (f\"## Response {iCount}\"))\n",
    "            display (Markdown (responseText)) \n",
    "        else:\n",
    "            print (f\"\\nResponse {iCount} is: {responseText}\\n\\n\")\n",
    "        iCount += 1\n",
    "        \n",
    "    return\n",
    "    \n",
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "    print (\"\\n\\n—BOJ—\\n\\n\")\n",
    "    testOpenAI ()\n",
    "    print (\"\\n\\n—EOJ—\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
